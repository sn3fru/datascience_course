{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferência 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Paradoxo de Simpson e regressão linear multipla **\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('hO8NMIU20Ck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paradoxo de Simpson\n",
    "\n",
    "<br>\n",
    "<img src=\"img/simpson_paradox.png\" width=\"450\" />\n",
    "<br>\n",
    "\n",
    "O paradoxo de Simpson (ou reversão de Simpson, efeito Yule-Simpson, paradoxo de amalgamação ou paradoxo de reversão) é um fenômeno em probabilidade e estatística, no qual uma tendência aparece em vários grupos diferentes de dados, mas desaparece ou reverte quando esses grupos são combinado.\n",
    "\n",
    "Este resultado é freqüentemente encontrado em estatísticas de ciências sociais e ciências médicas e é particularmente problemático quando dados de frequência são indevidamente dadas interpretações causais. Os elementos paradoxais desaparecem quando as relações causais são levadas em consideração. Ele tem sido usado para tentar informar o público não especialista ou público sobre o tipo de resultados enganosos que estatísticas erradas podem gerar. Martin Gardner escreveu um relato popular do paradoxo de Simpson em sua coluna de Jogos Matemáticos de março de 1976 na Scientific American.\n",
    "\n",
    "Edward H. Simpson descreveu esse fenômeno pela primeira vez em um artigo técnico em 1951, mas os estatísticos Karl Pearson et al., Em 1899, e Udny Yule, em 1903, mencionaram efeitos semelhantes anteriormente. O nome paradoxo de Simpson foi introduzido por Colin R. Blyth em 1972.\n",
    "\n",
    "## Exemplo: Tratamento de pedra nos rins\n",
    "\n",
    "Este é um exemplo da vida real de um estudo médico comparando as taxas de sucesso de dois tratamentos para cálculos renais.\n",
    "\n",
    "A tabela abaixo mostra as taxas de sucesso e o número de tratamentos para tratamentos envolvendo cálculos renais pequenos e grandes, onde o Tratamento A inclui todos os procedimentos cirúrgicos abertos e o Tratamento B é a nefrolitotomia percutânea (que envolve apenas uma pequena punção). Os números entre parênteses indicam o número de casos de sucesso sobre o tamanho total do grupo.\n",
    "\n",
    "<br>\n",
    "<img src=\"img/simpson_kidney.png\" width=\"450\" />\n",
    "<br>\n",
    "\n",
    "A conclusão paradoxal é que o tratamento A é mais eficaz quando usado em pedras pequenas, e também quando usado em pedras grandes, mas o tratamento B é mais eficaz quando se considera os dois tamanhos ao mesmo tempo. Neste exemplo, a variável \"à espreita\" (ou variável de confusão) é a gravidade do caso (representada pela tendência da decisão de tratamento dos médicos de favorecer B para casos menos graves), que não era previamente conhecida como importante até que seus efeitos fossem incluído.\n",
    "\n",
    "Qual tratamento é considerado melhor é determinado por uma desigualdade entre duas razões (sucessos / total). A inversão da desigualdade entre as razões, que cria o paradoxo de Simpson, acontece porque dois efeitos ocorrem juntos:\n",
    "\n",
    "Os tamanhos dos grupos, que são combinados quando a variável oculta é ignorada, são muito diferentes. Os médicos tendem a dar aos casos graves (pedras grandes) o melhor tratamento (A), e os casos mais leves (pedras pequenas) ao tratamento inferior (B). Portanto, os totais são dominados pelos grupos 3 e 2, e não pelos dois grupos muito menores 1 e 4.\n",
    "\n",
    "A variável oculta tem um grande efeito nas proporções; i.e., a taxa de sucesso é mais fortemente influenciada pela gravidade do caso do que pela escolha do tratamento. Portanto, o grupo de pacientes com cálculos grandes utilizando o tratamento A (grupo 3) piorou que o grupo com cálculos pequenos (grupos 1 e 2), mesmo se este último utilizou o tratamento inferior B (grupo 2).\n",
    "\n",
    "Com base nesses efeitos, o resultado paradoxal é visto pela supressão do efeito causal da gravidade do caso no tratamento bem-sucedido. O resultado paradoxal pode ser reformulado mais precisamente da seguinte forma: quando o tratamento menos eficaz (B) é aplicado com maior frequência a casos menos graves, pode parecer um tratamento mais eficaz.\n",
    "\n",
    "Vamos trabalhar com os dados para verificar a ocorrência do Paradoxo da Simpson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([['A','small', 81, 87],\n",
    "                   ['A','large', 192, 263],\n",
    "                   ['B','small', 234, 270],\n",
    "                   ['B','large', 55, 80],], columns=['treatment', 'kidney_stone_size', 'recovery', 'total'])   \n",
    "df\n",
    "#simpsons_paradox( df, 'recovery', 'total', 'treatment', 'kidney_stone_size' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total por tipo de tratamento\n",
    "\n",
    "Analisando os valores totais, podemos concluir que o tratamento B é mais bem sucedido que o tratamento A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stone_size = df.groupby('treatment').sum()\n",
    "df_stone_size['rate'] = df_stone_size.recovery/df_stone_size.total\n",
    "\n",
    "list = df_stone_size['rate'].tolist()\n",
    "rounded_list = [round(elem, 2) for elem in list]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "ax = df_stone_size['rate'].plot(kind='bar')\n",
    "ax.set_title('Total')\n",
    "ax.set_xlabel('Tratment Type')\n",
    "ax.set_ylabel('Sucess Rate')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make some labels.\n",
    "labels = [rounded_list[i] for i in range(len(rounded_list))]\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height, label,\n",
    "            ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parciais por tipo de tamanho da pedra (Large & Small)\n",
    "\n",
    "Por outro lado, analisando as parciais para os tipos de tamanho de pedra, constatamos que o tratamento A é mais bem sucedido que o tratamento B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_large = df[df.kidney_stone_size == 'large']\n",
    "df_large = df_large.set_index('treatment')\n",
    "df_large['rate'] = df_large.recovery/df_large.total\n",
    "\n",
    "list = df_large['rate'].tolist()\n",
    "rounded_list = [round(elem, 2) for elem in list]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "ax = df_large['rate'].plot(kind='bar')\n",
    "ax.set_title('Large')\n",
    "ax.set_xlabel('Large')\n",
    "ax.set_ylabel('Sucess Rate')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make some labels.\n",
    "labels = [rounded_list[i] for i in range(len(rounded_list))]\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height, label,\n",
    "            ha='center', va='bottom')\n",
    "    \n",
    "df_small = df[df.kidney_stone_size == 'small']\n",
    "df_small = df_small.set_index('treatment')\n",
    "df_small['rate'] = df_small.recovery/df_small.total\n",
    "\n",
    "list = df_small['rate'].tolist()\n",
    "rounded_list = [round(elem, 2) for elem in list]\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "ax = df_small['rate'].plot(kind='bar')\n",
    "ax.set_title('Small')\n",
    "ax.set_xlabel('Small')\n",
    "ax.set_ylabel('Sucess Rate')\n",
    "\n",
    "rects = ax.patches\n",
    "\n",
    "# Make some labels.\n",
    "labels = [rounded_list[i] for i in range(len(rounded_list))]\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height, label,\n",
    "            ha='center', va='bottom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para detecção do Paradoxo de Simpson, para o tratamento de pedras nos rins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect simpson's paradox\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def aggregate_data(df, conversion_col, treatment_col, segment_col):\n",
    "    \"\"\"\n",
    "    takes table of individual level data and aggregates it for simpsons paradox detection.\n",
    "    conversion_col is 1 if success, 0 else. \n",
    "    ex:\n",
    "    pd.DataFrame([\n",
    "        ['small', 'A', 1],\n",
    "        ['small', 'B', 0],\n",
    "        ['large', 'A', 1],\n",
    "        ['small', 'A', 1],\n",
    "        ['large', 'B', 0],\n",
    "        ['large', 'B', 0],\n",
    "    ], columns=['kidney_stone_size', 'treatment', 'recovery'])   \n",
    "    \"\"\"\n",
    "    df_ = df[[conversion_col, treatment_col, segment_col]]\n",
    "    gb = df_.groupby([segment_col, treatment_col]).agg(\n",
    "        [np.sum, lambda x: len(x)])\n",
    "    gb.columns = [conversion_col, \"total\"]\n",
    "\n",
    "    return gb.reset_index()\n",
    "\n",
    "\n",
    "def simpsons_paradox(df, conversion_col, total_col, treatment_col, segment_col):\n",
    "    \"\"\"\n",
    "    given a dataframe like:\n",
    "        pd.DataFrame([\n",
    "            ['small', 'A', 81, 87],\n",
    "            ['small', 'B', 234, 270],\n",
    "            ['large', 'A', 192, 263],\n",
    "            ['large', 'B', 55, 80],\n",
    "        ], columns=['kidney_stone_size', 'treatment', 'recovery', 'total'])   \n",
    "    will determine if simpsons paradox exists. Non Bayesian!\n",
    "    > simpsons_paradox( df, 'recovery', 'total', 'treatment', 'kidney_stone_size' )    \n",
    "    \"\"\"\n",
    "\n",
    "    # find global optimal:\n",
    "    gbs = df.groupby(treatment_col).sum()\n",
    "    print (\"## Global rates: \")\n",
    "    print (gbs[conversion_col] / gbs[total_col])\n",
    "    print\n",
    "    global_optimal = (gbs[conversion_col] / gbs[total_col]).argmax()\n",
    "\n",
    "    # check optimal via segments\n",
    "    df_ = df.set_index([segment_col, treatment_col])\n",
    "    rates = (df_[conversion_col] / df_[total_col]).unstack(-1)\n",
    "    print (\"## Local rates:\")\n",
    "    print (rates)\n",
    "    print\n",
    "    # find the local optimals\n",
    "    local_optimals = rates.apply(lambda x: x.argmax(), 1)\n",
    "\n",
    "    if local_optimals.unique().shape[0] > 1:\n",
    "        print (\"## Simpsons paradox not detected.\")\n",
    "        print (\"## Segmented rates do not have a consistent optimal choice\")\n",
    "        print (\"## Local optimals:\")\n",
    "        print (local_optimals)\n",
    "        print (\"## Global optimal: \", global_optimal)\n",
    "        return False\n",
    "\n",
    "    local_optimal = local_optimals.unique()[0]\n",
    "\n",
    "    print (\"## Global optimal: \", global_optimal)\n",
    "    print (\"## Local optimal: \", local_optimal)\n",
    "    if local_optimal != global_optimal:\n",
    "        print (\"## Simpsons Paradox detected.\")\n",
    "        return True\n",
    "\n",
    "    else:\n",
    "        print (\"## Simpsons paradox not detected.\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create some data, indentical to the data at\n",
    "    # http://en.wikipedia.org/wiki/Simpsons_paradox\n",
    "    d = []\n",
    "    d += ([('A', 'small', 1)] * 81)\n",
    "    d += ([('A', 'small', 0)] * (87 - 81))\n",
    "    d += ([('B', 'small', 0)] * (270 - 234))\n",
    "    d += ([('B', 'small', 1)] * (234))\n",
    "    d += ([('B', 'large', 1)] * (55))\n",
    "    d += ([('B', 'large', 0)] * (80 - 55))\n",
    "    d += ([('A', 'large', 0)] * (263 - 192))\n",
    "    d += ([('A', 'large', 1)] * (192))\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        d, columns=['treatment', 'kidney_stone_size', 'recovery'])\n",
    "    gb = aggregate_data(df, 'recovery', 'treatment', 'kidney_stone_size')\n",
    "simpsons_paradox(gb, 'recovery', 'total', 'treatment', 'kidney_stone_size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teste Bayesiano A / B com Python\n",
    "\n",
    "Qualquer pessoa que queira fazer um teste A / B sabe que as teorias estatísticas têm algo a dizer sobre se é A ou B que tem a melhor performance. O bom do teste Bayesiano A / B é aquele onde fica (relativamente) claro como tomar essa decisão.\n",
    "\n",
    "Vamos imaginar que temos uma experiência em execução, onde queremos saber se são as Alpacas (Llamas) ou os Ursos que apresentam melhor performance para a taxa de conversào de usuários na página de destino de nosso site. O teste A/B já é muito conhecido entre as pessoas que trabalham com dados e utilizam ferramentas no processo de tomada de decisão. \n",
    "\n",
    "Para o artigo original [clique aqui](https://medium.com/hockey-stick/tl-dr-bayesian-a-b-testing-with-python-c495d375db4d).\n",
    "\n",
    "<br>\n",
    "<img src=\"img/bear.png\" width=\"450\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A distribuição BETA\n",
    "\n",
    "Em teoria da probabilidade e estatística, a distribuição beta é uma família de distribuições de probabilidade contínuas definidas no intervalo [ 0 , 1 ] {\\displaystyle [0,1]} [0,1] parametrizado por dois parâmetros positivos, denotados por α {\\displaystyle \\alpha } \\alpha e β {\\displaystyle \\beta } \\beta, que aparecem como expoentes da variável aleatória e controlam o formato da distribuição.\n",
    "\n",
    "A distribuição beta tem sido aplicada para modelar o comportamento de variáveis aleatórias limitadas a intervalos de tamanho finito em uma grande quantidade de disciplinas.\n",
    "\n",
    "Em Inferência bayesiana, a distribuição beta é a distribuição conjugada a priori da distribuição de Bernoulli, distribuição binomial, distribuição binomial negativa e distribuição geométrica. Por exemplo, a distribuição beta pode ser usada na análise bayesiana para descrever conhecimentos iniciais sobre a probabilidade de sucesso assim como a probabilidade de que um veículo espacial vai completar uma missão especificada. A distribuição beta é um modelo conveniente para comportamento aleatório de porcentagens e proporções. \n",
    "\n",
    "Para saber mais sobre distribuição beta [clique aqui](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "\n",
    "\n",
    "### Utilizando a distribuição Beta em nosso experimento\n",
    "\n",
    "A idéia geral por trás do teste de taxa de conversão bayesiana é gerar duas distribuições que verifiquem todas as taxas possíveis e atualizá-las com informações sobre o desempenho do teste, ajustando a nossa expectativa da taxa mais representativa de acordo com os resultados.\n",
    "\n",
    "Podemos representar isso com a distribuição Beta que possui dois parâmetros: α, que representa as conversões bem-sucedidas e β, que representa as pessoas que saíram sem converter.\n",
    "\n",
    "Podemos pensar em α e β como probabilidades: 10 pra 1, 2 pra 3, e assim por diante. A única diferença é que 4 pra 6 representa um resultado mais representativo que, na mesma taxa de conversão, 2 pra 3.\n",
    "\n",
    "Antes de iniciarmos o experimento, não temos idéia se, entre Llamas ou Ursos, qual das opções é melhor para conversão.\n",
    "\n",
    "Digamos que, se não temos ideia alguma sobre qual das opções é melhor, para ambas qualquer taxa de conversão seria igualmente provável. Podemos representar isso com uma única conversão e uma única saída ou Beta (α = 1, β = 1).\n",
    "\n",
    "Graficamente a distribuição que se parece com o gráfico abaixo, que nos permite entender melhor o código e como plotar uma distribuição Beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando as bilbiotecas\n",
    "\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o gráfico\n",
    "fig, ax = plt.subplots(1, 1) \n",
    "\n",
    "# Gerando a distribuição\n",
    "beta_distribution = beta(1,1)\n",
    "\n",
    "# Plotando a destribuição\n",
    "\n",
    "x = np.linspace(0., 0.5, 1000)\n",
    "\n",
    "ax.plot(x, beta_distribution.pdf(x),color='red')\n",
    "ax.set(xlabel='conversion rate', ylabel='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo de plotagem da distribuição BETA\n",
    "\n",
    "Na realidade, sabemos um pouco sobre as taxas de conversão prováveis. Podemos usar essa informação para descartar resultados improváveis e acelerar nosso teste!\n",
    "\n",
    "Digamos que experimentamos Alpacas antes e descobrimos que eles convertem cerca de 16% do tempo. Para representar isso, podemos usar a distribuição Beta (16, 100-16). No entanto, podemos ser um pouco céticos sobre o desempenho, então vamos reduzi-la para Beta (8, 50-8).\n",
    "\n",
    "Ambas as distribuições são mostradas abaixo. Vale a pena brincar com os parâmetros destas funções Beta para entender melhor o nosso estudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o gráfico\n",
    "fig, ax = plt.subplots(1, 1) \n",
    "\n",
    "x = np.linspace(0.02, 0.3, 1000) \n",
    "\n",
    "# Gerando e plotando as distribuições\n",
    "alpacas1_distribution = beta(16, 100-16)  # conversão de 16 em 100 cliques\n",
    "alpacas2_distribution = beta(8, 50-8)     # conversão de 8 em 50 cliques\n",
    "\n",
    "ax.plot(x, alpacas1_distribution.pdf(x),label='Beta: 16 em 100',color='black')\n",
    "ax.plot(x, alpacas2_distribution.pdf(x),label='Beta: 8 em 50',color='red')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduzindo o teste A/B\n",
    "\n",
    "Agora vamos adicionar dados \"reais\" à essa experiência, porém vamos vamos ter que assumir uma premissa que ainda é desconhecida, os Ursos têm melhor performance do que as Alpacas.\n",
    "\n",
    "Vamos simular esta modelagem gerando um pequeno número de resultados aleatórios entre 0 e 1, depois escolhendo valores abaixo de um certo ponto de corte.\n",
    "\n",
    "O corte representa nossa taxa de conversão real e os valores abaixo dela são nossas conversões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total de observações (número de pessoas do univeerso amostral)\n",
    "people_in_branch = 50\n",
    "\n",
    "# Dados de controle são Alpaca, e dados de experimento são Ursos\n",
    "np.random.seed(42)\n",
    "control, experiment = np.random.rand(2, people_in_branch)\n",
    "\n",
    "# Número de sucassos das Alpacas (controle) de 16%\n",
    "c_successes = sum(control < 0.16)\n",
    "\n",
    "# Vamos assumir uma premissa que os Ursos tem uma performance 10% melhor que as Alpacas\n",
    "delta=1.1\n",
    "e_successes = sum(experiment < 0.16*delta)\n",
    "\n",
    "c_failures = people_in_branch - c_successes\n",
    "e_failures = people_in_branch - e_successes\n",
    "\n",
    "# Nossos pontos de partida para resultados do experimento\n",
    "prior_successes = 8\n",
    "prior_failures = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o comportamento dos dados iniciais\n",
    "\n",
    "Vamos dar uma olhada nos resultados da nossa experiência. A primeira coisa que fazemos é pegar nossa pequena quantidade de dados iniciais e adicionar nossos pontos de partida aos dois grupos do teste (Alpacas=controle e Ursos=experimento). Depois disso, geramos as distribuições posteriores e fazemos alguns gráficos dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o gráfico\n",
    "fig, ax = plt.subplots(1, 1) \n",
    "\n",
    "# Controle\n",
    "c_alpha, c_beta = c_successes + prior_successes, c_failures + prior_failures\n",
    "# Experimento\n",
    "e_alpha, e_beta = e_successes + prior_successes, e_failures + prior_failures\n",
    "\n",
    "x = np.linspace(0., 0.5, 1000) \n",
    "\n",
    "# Gerando e plotando as distribuições\n",
    "c_distribution = beta(c_alpha, c_beta)  # controle representa as Alpacas\n",
    "e_distribution = beta(e_alpha, e_beta)  # experimento representa os Ursos\n",
    "\n",
    "ax.plot(x, c_distribution.pdf(x))\n",
    "ax.plot(x, e_distribution.pdf(x))\n",
    "\n",
    "ax.plot(x, c_distribution.pdf(x),label='controle: alpacas',color='black')\n",
    "ax.plot(x, e_distribution.pdf(x),label='experimento: ursos',color='red')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "ax.set(xlabel='conversion rate', ylabel='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumentando o tamanho do espaço amostral\n",
    "\n",
    "O resultado do gráfico acima demonstra uma diferença entre os dois grupos, mas eles se sobrepõem tanto que é difícil dizer se esta diferença é estatísticamente significativa.\n",
    "Precisamos de mais dadso para poder tirar uma melhor conclusão, portanto vamos aumentar o tamanho do espaço amostral para 4000 pessoas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_people_in_branch = 4000\n",
    "\n",
    "# Controle são as Alpacas, experimento são os Ursos\n",
    "control, experiment = np.random.rand(2, more_people_in_branch)\n",
    "\n",
    "# Adicinando aos dados existentes\n",
    "# Performance das Alpacas (controle) de 16% ee a performance dos Ursos (experimento) é 10% superior\n",
    "\n",
    "c_successes += sum(control < 0.16)\n",
    "e_successes += sum(experiment < 0.16*delta)\n",
    "\n",
    "c_failures += more_people_in_branch - sum(control < 0.16)\n",
    "e_failures += more_people_in_branch - sum(experiment < 0.16*delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando o comportamento dos dados\n",
    "\n",
    "Vamos plotar as curvas de distribuição Beta para o controle (Alpacas) e o experimento (Ursos), e observar se existe uma diferença significativa entre oss dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para o gráfico\n",
    "fig, ax = plt.subplots(1, 1) \n",
    "\n",
    "# Controle\n",
    "c_alpha, c_beta = c_successes + prior_successes, c_failures + prior_failures\n",
    "\n",
    "# Experimento\n",
    "e_alpha, e_beta = e_successes + prior_successes, e_failures + prior_failures\n",
    "\n",
    "\n",
    "# Gerando e plotando as distribuições\n",
    "\n",
    "x = np.linspace(0., 0.5, 1000)\n",
    "\n",
    "c_distribution = beta(c_alpha, c_beta)\n",
    "e_distribution = beta(e_alpha, e_beta)\n",
    "\n",
    "ax.plot(x, c_distribution.pdf(x),label='controle: alpacas',color='Black')\n",
    "ax.plot(x, e_distribution.pdf(x),label='experimento: ursos',color='red')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "ax.set(xlabel='conversion rate', ylabel='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmação da hipótese\n",
    "\n",
    "Agora podemos dizer que a performance dos Ursos (experimento) está claramente melhor do que a performance das Alpacas (controle), pois as curvas apresentam uma área comum bastante reduzida devido ao aumento no tamanho das amostras e a diminuição da variância dos dados.\n",
    "Vamos calcular o p-valor para verificar o limite da diferença entre os dois grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments are x values so use ppf - the inverse of cdf\n",
    "print(c_distribution.ppf([0.025, 0.5, 0.975]))\n",
    "print(e_distribution.ppf([0.025, 0.5, 0.975]))\n",
    "\n",
    "# [ 0.14443947  0.15530981  0.16661068]\n",
    "# [ 0.15770843  0.16897057  0.18064618]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10000\n",
    "\n",
    "c_samples = pd.Series([c_distribution.rvs() for _ in range(sample_size)])\n",
    "e_samples = pd.Series([e_distribution.rvs() for _ in range(sample_size)])\n",
    "\n",
    "p_value = 1.0 - sum(e_samples > c_samples)/sample_size\n",
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o p-valor é menos que o limite de 0.05, podemos rejeitar a hipótese de que os dois grupos performam igual, e dizer que temos 95% de confiança que os Ursos (experimento) performam melhor que as Alpacas (controle)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curva da porcentagem acumulada dos dados do experimento divididos peleos dados de controle\n",
    "\n",
    "Esta curva revela o quanto a performance dos Ursos (experimento) é melhor em relação à performance das Alpacas (controle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ser = pd.Series(e_samples/c_samples)\n",
    "\n",
    "# Make the CDF\n",
    "ser = ser.sort_values()\n",
    "ser[len(ser)] = ser.iloc[-1] \n",
    "cum_dist = np.linspace(0., 1., len(ser))\n",
    "ser_cdf = pd.Series(cum_dist, index=ser)\n",
    "\n",
    "ax.plot(ser_cdf, color='red')\n",
    "ax.set(xlabel='Bears / Alpacas', ylabel='CDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
