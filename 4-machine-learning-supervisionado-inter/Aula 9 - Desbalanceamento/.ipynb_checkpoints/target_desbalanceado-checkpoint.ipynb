{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados Desbalanceados\n",
    "Usar o dataset de fraud (creditcard.csv)\n",
    "\n",
    "Dataset disponivel na aula 8.\n",
    "\n",
    "Criar um modelo preditivo que preveja fraude.\n",
    "\n",
    "A fraude é bastante rara.\n",
    "\n",
    "Use regressão logistica.\n",
    "\n",
    "Treine o modelo e calcule as métricas:\n",
    "f1, precisão, recall, auc.\n",
    "\n",
    "Agora pegue os casos de fraude e repita-os varias vezes (aumente a quantidade de treinamento para casos de fraude) e retreine o modelo.\n",
    "\n",
    "Por exemplo. Temos 1000 contratos normais e 10 fraudes, repita os contratos de fraude 30 vezes e teremos os mesmos 1000\n",
    "\n",
    "As métricas mudaram? Pq?\n",
    "\n",
    "Isso é o inicio da intuição de trabalhar com datasets desbalanceados e repetir a amostra varias vezes é uma tecnica (estupida) de oversampling.\n",
    "\n",
    "Outra possibilidade é diminuirmos a quantidade de amostras de não fraude para um número mais próximo do número de fraudes, essa técnica chamamos de undersampling.\n",
    "\n",
    "O que acontece com as métricas?\n",
    "\n",
    "\n",
    "Vamos usar uma técnica mais avançada agora. Primeiro vamos fazer um oversampling com uma técnica chamada SMOTE do pacote [imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn), que é uma extenção do nosso scikit-learn para datasets desbalanceados. Depois de fazer o tal oversampling vamos fazer um undersampling (OVERSAMPLING -> UNDERSAMPLING) e analisar as métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.998273\n",
       "1    0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roc_auc</td>\n",
       "      <td>0.952286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.674979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.538618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.428367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1\n",
       "0    roc_auc  0.952286\n",
       "1  precision  0.674979\n",
       "2     recall  0.538618\n",
       "3         f1  0.428367"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ['roc_auc', 'precision', 'recall', 'f1']\n",
    "\n",
    "metricas = []\n",
    "for metric in metrics:\n",
    "    metrica = cross_val_score(lr,\n",
    "                              df.drop('Class', axis=1),\n",
    "                              df['Class'],\n",
    "                              scoring = metric,\n",
    "                              cv=3)\n",
    "    \n",
    "    metricas.append([metric, metrica.mean()])\n",
    "    \n",
    "pd.DataFrame(metricas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = df[df['Class']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(492, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    frauds = pd.concat([frauds,\n",
    "                        frauds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503808, 31)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds2 = frauds.sample(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frauds2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nao_fraude = df[df.Class!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([frauds2,df_nao_fraude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9609284214982988"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica = cross_val_score(lr,\n",
    "                          newdf.drop('Class', axis=1),\n",
    "                          newdf['Class'],\n",
    "                          scoring = 'roc_auc',\n",
    "                          cv=3)\n",
    "\n",
    "metrica.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 31)\n",
      "(4920, 31)\n"
     ]
    }
   ],
   "source": [
    "df_fraud = df_copy[df_copy['Class']==1]\n",
    "df_nao_fraud = df_copy[df_copy['Class']==0].sample(df_fraud.shape[0]*10)\n",
    "\n",
    "print(df_fraud.shape)\n",
    "print(df_nao_fraud.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df_fraud, df_nao_fraud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8424499305968669"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrica = cross_val_score(lr,\n",
    "                          newdf.drop('Class', axis=1),\n",
    "                          newdf['Class'],\n",
    "                          scoring = 'roc_auc',\n",
    "                          cv=3)\n",
    "metrica.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\sn3fru\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\sn3fru\\anaconda3\\lib\\site-packages (from imblearn) (0.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sn3fru\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sn3fru\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.14.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sn3fru\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 30) (284807,)\n"
     ]
    }
   ],
   "source": [
    "x = df_copy.drop('Class', axis=1)\n",
    "y = df_copy['Class']\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 492\n",
      "Before OverSampling, counts of label '0': 284315 \n",
      "\n",
      "After OverSampling, the shape of train_X: (568630, 30)\n",
      "After OverSampling, the shape of train_y: (568630,) \n",
      "\n",
      "After OverSampling, counts of label '1': 284315\n",
      "After OverSampling, counts of label '0': 284315\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_sample(x, y)\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrica = cross_val_score(lr,\n",
    "                          X_train_res,\n",
    "                          y_train_res,\n",
    "                          scoring = 'roc_auc',\n",
    "                          cv=10)\n",
    "metrica.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
